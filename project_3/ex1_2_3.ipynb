{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import time\n",
    "RAND_SEED = 10 # good for kalman\n",
    "# RAND_SEED = 12 # bad for kalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamics of the pendulum\n",
    "@dataclass\n",
    "class Gaussian:\n",
    "    mean: np.ndarray\n",
    "    cov: np.ndarray\n",
    "\n",
    "\n",
    "DT = 0.01\n",
    "G = sp.constants.g\n",
    "\n",
    "\n",
    "def xkp1(xk: np.ndarray) -> np.ndarray:\n",
    "    return np.array([xk[0] + xk[1]*DT,\n",
    "                     xk[1] - G*np.sin(xk[0])*DT])\n",
    "\n",
    "\n",
    "def xkp1_vect(xk, dt=DT):\n",
    "    out = np.zeros(xk.shape)\n",
    "    out[:, 0] = xk[:, 0]+xk[:, 1]*dt\n",
    "    out[:, 1] = xk[:, 1] - G*np.sin(xk[:, 0])*dt\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_q() -> np.ndarray:\n",
    "    QC = 0.1\n",
    "    return np.array([[QC*DT**3/3, QC*DT**2/2],\n",
    "                     [QC*DT**2/2, QC*DT]])\n",
    "\n",
    "\n",
    "def yk(xk: np.ndarray) -> np.ndarray:\n",
    "    return np.array([np.sin(xk[0])])\n",
    "\n",
    "\n",
    "def yk_vect(xk):\n",
    "    out = np.zeros((xk.shape[0], 1))\n",
    "    out[:, 0] = np.sin(xk[:, 0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the problem\n",
    "\n",
    "@dataclass\n",
    "class Observations:\n",
    "    times: np.ndarray\n",
    "    obs_ind: np.ndarray  # index of times that are observed\n",
    "    obs: np.ndarray\n",
    "    names: List[str]\n",
    "\n",
    "\n",
    "def generate_truth(x0: np.ndarray, dt: float, N: int) -> Observations:\n",
    "    times = np.zeros(N+1)  # from 0 to N times\n",
    "    xk = np.zeros((N+1, 2))  # state at each time\n",
    "    xk[0] = x0\n",
    "    for i in range(1, N+1):\n",
    "        times[i] = i*dt\n",
    "        xk[i] = xkp1(xk[i-1])\n",
    "    return Observations(times, np.arange(N+1), xk, [r'$x_1$ true', r'$x_2$ true'])\n",
    "\n",
    "\n",
    "def generate_meas(truth: Observations, delta: float, Q: np.ndarray, R: float) -> Observations:\n",
    "    process_std = np.linalg.cholesky(Q)\n",
    "    measure_std = np.sqrt(R)\n",
    "    measure_ind = np.arange(delta+1, truth.obs_ind[-1]+1, delta)\n",
    "    obs = np.zeros((measure_ind.size, 1))\n",
    "    obs_ind = 0\n",
    "    for i in measure_ind:\n",
    "        obs[obs_ind] = yk(truth.obs[i] +\n",
    "                          np.dot(process_std, np.random.randn(2))) + measure_std*np.random.randn()\n",
    "        obs_ind += 1\n",
    "    return Observations(truth.times, measure_ind, obs, [r'$x_1$ obs'])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MeasurementData:\n",
    "    delta: float\n",
    "    R: float\n",
    "    obs: Observations\n",
    "\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "N = 500\n",
    "X0 = np.array([1.5, 0])\n",
    "prior = Gaussian(X0, np.eye(2))\n",
    "Q = get_q()\n",
    "proc_noise = Gaussian(np.zeros(2), Q)\n",
    "proc_cov_inv = np.linalg.pinv(proc_noise.cov)\n",
    "proc_cov_sqrtL = np.linalg.cholesky(proc_noise.cov)\n",
    "truth = generate_truth(X0, DT, N)\n",
    "deltas = [5, 10, 20, 40]\n",
    "Rs = [0.001, 0.01, 0.1, 1]\n",
    "data = []\n",
    "for delta in deltas:\n",
    "    for R in Rs:\n",
    "        data.append(MeasurementData(delta, R, generate_meas(truth, delta, Q, R)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup helper functions\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KFTracker:\n",
    "    means: np.ndarray\n",
    "    covs: np.ndarray\n",
    "    stds: np.ndarray\n",
    "\n",
    "\n",
    "def plot_data_and_truth(fignum, title, true_obs: Observations, data_obs: List[MeasurementData],\n",
    "                        kft: Optional[List[KFTracker]] = None) -> plt.figure:\n",
    "\n",
    "    colors = ['red', 'blue']\n",
    "    plt.close(fignum)\n",
    "    fig, axs = plt.subplots(4, 4, num=fignum, figsize=(15, 15), sharex=True, sharey=True)\n",
    "    fig.tight_layout(pad=3)\n",
    "    fig.suptitle(title, fontsize=18, y=1.0)\n",
    "    for i in range(len(data_obs)):\n",
    "        ax = axs[i//4, i % 4]\n",
    "        ax.plot(true_obs.times, true_obs.obs[:, 0], color=colors[0], label=true_obs.names[0])\n",
    "        ax.plot(true_obs.times, true_obs.obs[:, 1], color=colors[1], label=true_obs.names[1])\n",
    "\n",
    "        if kft is not None:\n",
    "            ax.plot(data_obs[i].obs.times, kft[i].means[:, 0],\n",
    "                    '--', color=colors[0], label=r'$x_1$ est')\n",
    "            ax.plot(data_obs[i].obs.times, kft[i].means[:, 1],\n",
    "                    '--', color=colors[1], label=r'$x_2$ est')\n",
    "            ax.fill_between(data_obs[i].obs.times,\n",
    "                            kft[i].means[:, 0] - 2 * kft[i].stds[:, 0],\n",
    "                            kft[i].means[:, 0] + 2 * kft[i].stds[:, 0],\n",
    "                            color=colors[0], alpha=0.3)\n",
    "            ax.fill_between(data_obs[i].obs.times,\n",
    "                            kft[i].means[:, 1] - 2 * kft[i].stds[:, 1],\n",
    "                            kft[i].means[:, 1] + 2 * kft[i].stds[:, 1],\n",
    "                            color=colors[1], alpha=0.3)\n",
    "            ax.set_title(\n",
    "                r'$\\delta$=' +\n",
    "                f'{data_obs[i].delta};R={data_obs[i].R};MSE={mean_sq_error(true_obs, kft[i]):.3f}',\n",
    "                fontsize=12)\n",
    "        else:\n",
    "            ax.set_title(r'$\\delta$='+f'{data_obs[i].delta};R={data_obs[i].R}', fontsize=12)\n",
    "\n",
    "        ax.plot(data_obs[i].obs.times[data_obs[i].obs.obs_ind], data_obs[i].obs.obs,\n",
    "                'o', color=colors[0], alpha=0.4, label=data_obs[i].obs.names[0])\n",
    "\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0f}'))\n",
    "\n",
    "    h, l = axs[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(h, l, loc='lower right', fontsize=12, ncol=3)\n",
    "    fig.supxlabel('Time', fontsize=14)\n",
    "    fig.supylabel('State Estimate', fontsize=14)\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    fig.savefig(f'figs/{title}.svg')\n",
    "    plt.show()\n",
    "    return fignum + 1\n",
    "\n",
    "\n",
    "def mean_sq_error(truth: Observations, results: KFTracker) -> float:\n",
    "    return np.mean((truth.obs - results.means)**2)\n",
    "\n",
    "\n",
    "def print_mean_sq_error(truth: Observations, results: KFTracker, data: list[MeasurementData]) -> float:\n",
    "    for result, obs in zip(results, data):\n",
    "        mse = mean_sq_error(truth, result)\n",
    "        print(f'delta={obs.delta} \\tR={obs.R}\\tMSE={mse:.1e}')\n",
    "\n",
    "\n",
    "fignum = 1\n",
    "\n",
    "fignum = plot_data_and_truth(fignum, 'Pendulum trajectories and data', truth, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# linearized dynamics\n",
    "\n",
    "\n",
    "def Ak(xk: np.ndarray) -> np.ndarray:\n",
    "    return np.array([[1, DT],\n",
    "                     [-G*np.cos(xk[0])*DT, 1]])\n",
    "\n",
    "\n",
    "def Hk(xk: np.ndarray) -> np.ndarray:\n",
    "    return np.array([[np.cos(xk[0]), 0]]).reshape(1, 2)\n",
    "\n",
    "# extended kalman filter\n",
    "\n",
    "\n",
    "def ekf_predict(phi: Callable[[np.ndarray], np.ndarray], A: np.ndarray,\n",
    "                X: Gaussian, xi: Gaussian) -> Gaussian:\n",
    "    \"\"\"Linear Prediction Step: Propagate uncertainty for one time step.\n",
    "\n",
    "    X_{k+1} = A X_{k} + xi, xi sim mathcal{N}{noise_mean, noise_cov}\n",
    "    X_{k} sim mathcal{N}(mean, cov)\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    phi: dynamics function\n",
    "    A: (d, d), linearized dynamics\n",
    "    X: Gaussian\n",
    "    xi: Gaussian\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Gaussian random variable for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    pred_mean = phi(X.mean) + xi.mean\n",
    "    pred_cov = np.dot(A, np.dot(X.cov, A.T)) + xi.cov\n",
    "    return Gaussian(pred_mean, pred_cov)\n",
    "\n",
    "\n",
    "def ekf_update(data: np.ndarray, h: Callable[[np.ndarray], np.ndarray], H: np.ndarray,\n",
    "               X: Gaussian, eta: Gaussian) -> Gaussian:\n",
    "    \"\"\"Linear Gaussian Inverse Problem.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: (N) array of data\n",
    "    h: observation function\n",
    "    H: (m, d), linearized observation model\n",
    "    X: Gaussian\n",
    "    eta: Gaussian\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Gaussian Posterior\n",
    "    \"\"\"\n",
    "    mu = h(X.mean) + eta.mean\n",
    "    U = np.dot(X.cov, H.T)\n",
    "    S = np.dot(H, U) + eta.cov\n",
    "\n",
    "    update_mean = X.mean + np.dot(U, np.linalg.solve(S, data - mu))\n",
    "    update_cov = X.cov - np.dot(U, np.linalg.solve(S, U.T))\n",
    "    return Gaussian(update_mean, update_cov)\n",
    "\n",
    "\n",
    "def extended_kalman_filter(data: Observations,\n",
    "                           phi: Callable[[np.ndarray], np.ndarray],\n",
    "                           A: Callable[[np.ndarray], np.ndarray],\n",
    "                           h: Callable[[np.ndarray], np.ndarray],\n",
    "                           H: Callable[[np.ndarray], np.ndarray],\n",
    "                           prior: Gaussian,\n",
    "                           xi: Gaussian, eta: Gaussian) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"The Kalman filter.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: (N, m), N is the number of time steps, m is the size of the observations\n",
    "    phi: dynamics function\n",
    "    A: linearized dynamics function\n",
    "    h: observation function\n",
    "    H: linearized observation function\n",
    "    prior: prior Gaussian\n",
    "    xi: process noise\n",
    "    eta: measurement noise\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    KFTracker\n",
    "    \"\"\"\n",
    "\n",
    "    num_steps = data.times.shape[0]\n",
    "\n",
    "    d = prior.mean.shape[0]\n",
    "    mean_store = np.zeros((num_steps, d))\n",
    "    mean_store[0, :] = np.copy(prior.mean)\n",
    "    cov_store = np.zeros((num_steps, d, d))\n",
    "    cov_store[0, :, :] = np.copy(prior.cov)\n",
    "\n",
    "    std_store = np.zeros((num_steps, d))\n",
    "    std_store[0, :] = np.sqrt(np.diag(cov_store[0, :, :]))\n",
    "\n",
    "    # Loop over all time steps\n",
    "    Xnext = prior\n",
    "    on_obs = 0\n",
    "    for ii in range(1, num_steps):\n",
    "        # Prediction\n",
    "        Xpred = ekf_predict(phi, A(Xnext.mean), Xnext, xi)\n",
    "\n",
    "        # We have an observation so an update must occur\n",
    "        if on_obs < data.obs_ind.shape[0] and ii == data.obs_ind[on_obs]:\n",
    "            y = data.obs[on_obs]\n",
    "            on_obs += 1\n",
    "\n",
    "            # Update\n",
    "            Xup = ekf_update(y, h, H(Xpred.mean), Xpred, eta)\n",
    "            Xnext = Xup\n",
    "\n",
    "        else:\n",
    "            Xnext = Xpred\n",
    "\n",
    "        mean_store[ii, :] = np.copy(Xnext.mean)\n",
    "        cov_store[ii, :, :] = np.copy(Xnext.cov)\n",
    "        std_store[ii, :] = np.sqrt(np.diag(cov_store[ii, :, :]))\n",
    "\n",
    "    return KFTracker(mean_store, cov_store, std_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscented kalman filter of order 3\n",
    "\n",
    "def unscented_points(X: Gaussian, alg: str = 'chol', alpha: float = 1,\n",
    "                     beta: float = 0, kappa: float = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate unscented points\"\"\"\n",
    "    dim = X.cov.shape[0]\n",
    "    lam = alpha*alpha*(dim + kappa) - dim\n",
    "    if alg == \"chol\":\n",
    "        L = np.linalg.cholesky(X.cov)\n",
    "    elif alg == \"svd\":\n",
    "        u, s, v = np.linalg.svd(X.cov)\n",
    "        L = np.dot(u, np.sqrt(np.diag(s)))\n",
    "    pts = np.zeros((2*dim+1, dim))\n",
    "    pts[0, :] = X.mean\n",
    "    for ii in range(1, dim+1):\n",
    "        pts[ii, :] = X.mean + np.sqrt(dim + lam)*L[:, ii-1]\n",
    "        pts[ii+dim, :] = X.mean - np.sqrt(dim + lam)*L[:, ii-1]\n",
    "    w_m = np.zeros((2*dim+1, 1))  # just 1 weight per point\n",
    "    w_c = np.zeros((2*dim+1, 1))  # just 1 weight per point\n",
    "    w_m[0] = lam / (dim + lam)\n",
    "    w_m[1:] = 1/2 / (dim + lam)\n",
    "    w_c[0] = lam / (dim + lam) + (1 - alpha*alpha + beta)\n",
    "    w_c[1:] = 1/2 / (dim + lam)\n",
    "    return pts, w_m, w_c\n",
    "\n",
    "\n",
    "def ukf_predict(phi: Callable[[np.ndarray], np.ndarray], X: Gaussian, xi: Gaussian,\n",
    "                alpha: float = 1, beta: float = 0, kappa: float = 0) -> Gaussian:\n",
    "    \"\"\"Prediction Step: Propagate uncertainty for one time step.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    phi: dynamics function\n",
    "    X: Gaussian\n",
    "    xi: Gaussian\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Gaussian random variable for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    dims_x = X.mean.shape[0]\n",
    "    u, w_m, w_c = unscented_points(X, alpha=alpha, beta=beta, kappa=kappa)\n",
    "    num_u = u.shape[0]\n",
    "\n",
    "    # pred_mean = phi(X.mean) + xi.mean\n",
    "    pred_state = phi(u)\n",
    "    pred_mean = np.sum(pred_state * w_m, axis=0)\n",
    "\n",
    "    # pred_cov = np.dot(A, np.dot(X.cov, A.T)) + xi.cov\n",
    "    pred_cov = np.zeros((dims_x, dims_x)) + xi.cov\n",
    "    std_state = pred_state - pred_mean\n",
    "    for i in range(num_u):\n",
    "        pred_cov += np.outer(std_state[i, :], std_state[i, :]) * w_c[i]\n",
    "\n",
    "    return Gaussian(pred_mean, pred_cov)\n",
    "\n",
    "\n",
    "def ukf_update(data: np.ndarray, h: Callable[[np.ndarray], np.ndarray], X: Gaussian,\n",
    "               eta: Gaussian, alpha: float = 1, beta: float = 0, kappa: float = 0) -> Gaussian:\n",
    "    \"\"\"Gaussian Inverse Problem\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: (N) array of data\n",
    "    h: observation function\n",
    "    X: Gaussian\n",
    "    eta: Gaussian\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Gaussian Posterior\n",
    "    \"\"\"\n",
    "\n",
    "    dims_x = X.mean.shape[0]\n",
    "    dims_data = 1\n",
    "    u, w_m, w_c = unscented_points(X, alpha=alpha, beta=beta, kappa=kappa)\n",
    "    num_u = u.shape[0]\n",
    "\n",
    "    # mu = h(X.mean) + eta.mean\n",
    "    pred_meas = h(u)\n",
    "    mu = np.sum(pred_meas * w_m, axis=0)\n",
    "\n",
    "    # U = np.dot(X.cov, H.T)\n",
    "    std_state = u - X.mean\n",
    "    std_meas = pred_meas - mu\n",
    "    U = np.zeros((dims_x, dims_data))\n",
    "    for i in range(num_u):\n",
    "        U += np.outer(std_state[i, :], std_meas[i, :]) * w_c[i]\n",
    "\n",
    "    # S = np.dot(H, U) + eta.cov\n",
    "    S = np.zeros((dims_data, dims_data)) + eta.cov\n",
    "    for i in range(num_u):\n",
    "        S += np.outer(std_meas[i, :], std_meas[i, :]) * w_c[i]\n",
    "\n",
    "    update_mean = X.mean + np.dot(U, np.linalg.solve(S, data - mu))\n",
    "    update_cov = X.cov - np.dot(U, np.linalg.solve(S, U.T))\n",
    "    return Gaussian(update_mean, update_cov)\n",
    "\n",
    "\n",
    "def unscented_kalman_filter(data: Observations,\n",
    "                            phi: Callable[[np.ndarray], np.ndarray],\n",
    "                            h: Callable[[np.ndarray], np.ndarray],\n",
    "                            prior: Gaussian,\n",
    "                            xi: Gaussian, eta: Gaussian,\n",
    "                            alpha: float = 1, beta: float = 0, kappa: float = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"The Kalman filter.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: (N, m), N is the number of time steps, m is the size of the observations\n",
    "    phi: dynamics function\n",
    "    h: observation function\n",
    "    prior: prior Gaussian\n",
    "    xi: process noise\n",
    "    eta: measurement noise\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    KFTracker\n",
    "    \"\"\"\n",
    "\n",
    "    num_steps = data.times.shape[0]\n",
    "\n",
    "    d = prior.mean.shape[0]\n",
    "    mean_store = np.zeros((num_steps, d))\n",
    "    mean_store[0, :] = np.copy(prior.mean)\n",
    "    cov_store = np.zeros((num_steps, d, d))\n",
    "    cov_store[0, :, :] = np.copy(prior.cov)\n",
    "\n",
    "    std_store = np.zeros((num_steps, d))\n",
    "    std_store[0, :] = np.sqrt(np.diag(cov_store[0, :, :]))\n",
    "\n",
    "    # Loop over all time steps\n",
    "    Xnext = prior\n",
    "    on_obs = 0\n",
    "    for ii in range(1, num_steps):\n",
    "        # Prediction\n",
    "        Xpred = ukf_predict(phi, Xnext, xi, alpha, beta, kappa)\n",
    "\n",
    "        # We have an observation so an update must occur\n",
    "        if on_obs < data.obs_ind.shape[0] and ii == data.obs_ind[on_obs]:\n",
    "            y = data.obs[on_obs]\n",
    "            on_obs += 1\n",
    "\n",
    "            # Update\n",
    "            Xup = ukf_update(y, h, Xpred, eta, alpha, beta, kappa)\n",
    "            Xnext = Xup\n",
    "\n",
    "        else:\n",
    "            Xnext = Xpred\n",
    "\n",
    "        mean_store[ii, :] = np.copy(Xnext.mean)\n",
    "        cov_store[ii, :, :] = np.copy(Xnext.cov)\n",
    "        std_store[ii, :] = np.sqrt(np.diag(cov_store[ii, :, :]))\n",
    "\n",
    "    return KFTracker(mean_store, cov_store, std_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaus-hermite kalman filter\n",
    "\n",
    "def gh_oned(num_pts=2):\n",
    "    \"\"\"Gauss-hermite quadrature in 1D\"\"\"\n",
    "    A = np.zeros((num_pts, num_pts))\n",
    "    for ii in range(num_pts):\n",
    "        # print(\"ii \", ii, ii==0, ii==(order-1))\n",
    "        row = ii+1\n",
    "        if ii == 0:\n",
    "            A[ii, ii+1] = np.sqrt(row)\n",
    "            A[ii+1, ii] = np.sqrt(row)\n",
    "        elif ii == (num_pts-1):\n",
    "            A[ii-1, ii] = np.sqrt(ii)\n",
    "        else:\n",
    "            A[ii, ii+1] = np.sqrt(row)\n",
    "            A[ii+1, ii] = np.sqrt(row)\n",
    "    pts, evec = np.linalg.eig(A)\n",
    "    devec = np.dot(evec.T, evec)\n",
    "    wts = evec[0, :]**2\n",
    "\n",
    "    return pts, wts\n",
    "\n",
    "\n",
    "def tensorize(nodes):\n",
    "    \"\"\"Tensorize nodes to obtain twod\"\"\"\n",
    "    n1d = nodes.shape[0]\n",
    "    twodnodes = np.zeros((n1d*n1d, 2))\n",
    "    ind = 0\n",
    "    for ii in range(n1d):\n",
    "        for jj in range(n1d):\n",
    "            twodnodes[ind, :] = np.array([nodes[ii], nodes[jj]])\n",
    "            ind += 1\n",
    "    return twodnodes\n",
    "\n",
    "\n",
    "def gauss_hermite(dim, num_pts=2):\n",
    "    \"\"\"Gauss-hermite quadrature in 2D\"\"\"\n",
    "    assert dim == 2, \"Tensorize only implemented for dim=2\"\n",
    "    pts, weights = gh_oned(num_pts)\n",
    "    ptsT = tensorize(pts)\n",
    "    weightsT = tensorize(weights)\n",
    "    weightsT = np.prod(weightsT, axis=1)\n",
    "    return ptsT, weightsT.reshape(-1, 1)  # reshape so theyre column vectors like ptsT\n",
    "\n",
    "\n",
    "def rotate_points(points: int, X: Gaussian, alg: str = \"chol\"):\n",
    "    \"\"\"Rotating points from standard gaussian to target Gaussian\"\"\"\n",
    "    if alg == \"chol\":\n",
    "        L = np.linalg.cholesky(X.cov)\n",
    "    elif alg == \"svd\":\n",
    "        u, s, v = np.linalg.svd(X.cov)\n",
    "        L = np.dot(u, np.sqrt(np.diag(s)))\n",
    "\n",
    "    new_points = np.zeros(points.shape)\n",
    "    for ii in range(points.shape[0]):\n",
    "        new_points[ii, :] = X.mean + np.dot(L, points[ii, :].T)\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def ghkf_predict(phi: Callable[[np.ndarray], np.ndarray], X: Gaussian,\n",
    "                 xi: Gaussian, order: int) -> Gaussian:\n",
    "    \"\"\"Prediction Step: Propagate uncertainty for one time step.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    phi: dynamics function\n",
    "    X: Gaussian\n",
    "    xi: Gaussian\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Gaussian random variable for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    dims_x = X.mean.shape[0]\n",
    "    u, w = gauss_hermite(dims_x, num_pts=order)  # get points from std gauss\n",
    "    u = rotate_points(u, X)  # rotate points to target Gaussian\n",
    "    num_u = u.shape[0]\n",
    "\n",
    "    # pred_mean = phi(X.mean) + xi.mean\n",
    "    pred_state = phi(u)\n",
    "    pred_mean = np.sum(pred_state * w, axis=0)\n",
    "\n",
    "    # pred_cov = np.dot(A, np.dot(X.cov, A.T)) + xi.cov\n",
    "    pred_cov = np.zeros((dims_x, dims_x)) + xi.cov\n",
    "    std_state = pred_state - pred_mean\n",
    "    for i in range(num_u):\n",
    "        pred_cov += np.outer(std_state[i, :], std_state[i, :]) * w[i]\n",
    "\n",
    "    return Gaussian(pred_mean, pred_cov)\n",
    "\n",
    "\n",
    "def ghkf_update(data: np.ndarray, h: Callable[[np.ndarray], np.ndarray],\n",
    "                X: Gaussian, eta: Gaussian, order: int) -> Gaussian:\n",
    "    \"\"\"Gaussian Inverse Problem\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: (N) array of data\n",
    "    h: observation function\n",
    "    X: Gaussian\n",
    "    eta: Gaussian\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Gaussian Posterior\n",
    "    \"\"\"\n",
    "\n",
    "    dims_x = X.mean.shape[0]\n",
    "    dims_data = 1\n",
    "    u, w = gauss_hermite(dims_x, num_pts=order)  # get points from std gauss\n",
    "    u = rotate_points(u, X)  # rotate points to target Gaussian\n",
    "    num_u = u.shape[0]\n",
    "\n",
    "    # mu = h(X.mean) + eta.mean\n",
    "    pred_meas = h(u)\n",
    "    mu = np.sum(pred_meas * w, axis=0)\n",
    "\n",
    "    # U = np.dot(X.cov, H.T)\n",
    "    std_state = u - X.mean\n",
    "    std_meas = pred_meas - mu\n",
    "    U = np.zeros((dims_x, dims_data))\n",
    "    for i in range(num_u):\n",
    "        U += np.outer(std_state[i, :], std_meas[i, :]) * w[i]\n",
    "\n",
    "    # S = np.dot(H, U) + eta.cov\n",
    "    S = np.zeros((dims_data, dims_data)) + eta.cov\n",
    "    for i in range(num_u):\n",
    "        S += np.outer(std_meas[i, :], std_meas[i, :]) * w[i]\n",
    "\n",
    "    update_mean = X.mean + np.dot(U, np.linalg.solve(S, data - mu))\n",
    "    update_cov = X.cov - np.dot(U, np.linalg.solve(S, U.T))\n",
    "    return Gaussian(update_mean, update_cov)\n",
    "\n",
    "\n",
    "def gauss_hermite_kalman_filter(data: Observations,\n",
    "                                phi: Callable[[np.ndarray], np.ndarray],\n",
    "                                h: Callable[[np.ndarray], np.ndarray],\n",
    "                                prior: Gaussian,\n",
    "                                xi: Gaussian, eta: Gaussian,\n",
    "                                order: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"The Kalman filter.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: (N, m), N is the number of time steps, m is the size of the observations\n",
    "    phi: dynamics function\n",
    "    h: observation function\n",
    "    prior: prior Gaussian\n",
    "    xi: process noise\n",
    "    eta: measurement noise\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    KFTracker\n",
    "    \"\"\"\n",
    "\n",
    "    num_steps = data.times.shape[0]\n",
    "\n",
    "    d = prior.mean.shape[0]\n",
    "    mean_store = np.zeros((num_steps, d))\n",
    "    mean_store[0, :] = np.copy(prior.mean)\n",
    "    cov_store = np.zeros((num_steps, d, d))\n",
    "    cov_store[0, :, :] = np.copy(prior.cov)\n",
    "\n",
    "    std_store = np.zeros((num_steps, d))\n",
    "    std_store[0, :] = np.sqrt(np.diag(cov_store[0, :, :]))\n",
    "\n",
    "    # Loop over all time steps\n",
    "    Xnext = prior\n",
    "    on_obs = 0\n",
    "    for ii in range(1, num_steps):\n",
    "        # Prediction\n",
    "        Xpred = ghkf_predict(phi, Xnext, xi, order)\n",
    "\n",
    "        # We have an observation so an update must occur\n",
    "        if on_obs < data.obs_ind.shape[0] and ii == data.obs_ind[on_obs]:\n",
    "            y = data.obs[on_obs]\n",
    "            on_obs += 1\n",
    "\n",
    "            # Update\n",
    "            Xup = ghkf_update(y, h, Xpred, eta, order)\n",
    "            Xnext = Xup\n",
    "\n",
    "        else:\n",
    "            Xnext = Xpred\n",
    "\n",
    "        mean_store[ii, :] = np.copy(Xnext.mean)\n",
    "        cov_store[ii, :, :] = np.copy(Xnext.cov)\n",
    "        std_store[ii, :] = np.sqrt(np.diag(cov_store[ii, :, :]))\n",
    "\n",
    "    return KFTracker(mean_store, cov_store, std_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all the kalma filters\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "ekf_results = []\n",
    "ekf_start = time.time()\n",
    "for obs in data:\n",
    "    ekf_results.append(extended_kalman_filter(obs.obs, xkp1, Ak, yk,\n",
    "                       Hk, prior, proc_noise, Gaussian(0, obs.R)))\n",
    "ekf_time = time.time() - ekf_start\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "ukf_results = []\n",
    "ukf_start = time.time()\n",
    "for obs in data:\n",
    "    ukf_results.append(unscented_kalman_filter(obs.obs, xkp1_vect,\n",
    "                       yk_vect, prior, proc_noise, Gaussian(0, obs.R)))\n",
    "ukf_time = time.time() - ukf_start\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "ghkf3_results = []\n",
    "ghkf3_start = time.time()\n",
    "for obs in data:\n",
    "    ghkf3_results.append(gauss_hermite_kalman_filter(obs.obs, xkp1_vect,\n",
    "                                                     yk_vect, prior, proc_noise, Gaussian(0, obs.R), 3))\n",
    "ghkf3_time = time.time() - ghkf3_start\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "ghkf5_results = []\n",
    "ghkf5_start = time.time()\n",
    "for obs in data:\n",
    "    ghkf5_results.append(gauss_hermite_kalman_filter(obs.obs, xkp1_vect,\n",
    "                                                     yk_vect, prior, proc_noise, Gaussian(0, obs.R), 5))\n",
    "ghkf5_time = time.time() - ghkf5_start\n",
    "\n",
    "\n",
    "print('** Timing')\n",
    "print(f'EKF:\\t{ekf_time:.3f} s')\n",
    "print(f'UKF:\\t{ukf_time:.3f} s')\n",
    "print(f'GHKF 3:\\t{ghkf3_time:.3f} s')\n",
    "print(f'GHKF 5:\\t{ghkf5_time:.3f} s')\n",
    "\n",
    "print('** Mean Squared Error')\n",
    "print(f\"EKF:\")\n",
    "print_mean_sq_error(truth, ekf_results, data)\n",
    "print_mean_sq_error(truth, ukf_results, data)\n",
    "print_mean_sq_error(truth, ghkf3_results, data)\n",
    "print_mean_sq_error(truth, ghkf5_results, data)\n",
    "\n",
    "# plots\n",
    "fignum = plot_data_and_truth(fignum, 'Pendulum trajectories (EKF)', truth, data, ekf_results)\n",
    "fignum = plot_data_and_truth(fignum, 'Pendulum trajectories (UKF)', truth, data, ukf_results)\n",
    "fignum = plot_data_and_truth(\n",
    "    fignum, 'Pendulum trajectories (3rd order GHKF)', truth, data, ghkf3_results)\n",
    "fignum = plot_data_and_truth(\n",
    "    fignum, 'Pendulum trajectories (5th order GHKF)', truth, data, ghkf5_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particle filter\n",
    "\n",
    "def resample(Nsamples, samples, weights):\n",
    "    \"\"\"Generate *Nsamples* samples from an empirical distribution defined by *samples* and *weights*\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    Nsamples: integer, number of samples to generate\n",
    "    samples: (N, d) array of N samples of dimension d that form the empirical distribution\n",
    "    weights: (N, ) array of N weights\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    samples_out: (Nsamples, d) new samples\n",
    "    weights_out: (Nsamples, ) new weights equal to 1 / N\n",
    "    \"\"\"\n",
    "\n",
    "    N = samples.shape[0]  # get number of points that make up the empirical distribution\n",
    "    rr = np.arange(N)  # get an ordered set of numbers from 0 to N-1\n",
    "\n",
    "    # Randomly choose the integers (with replacement) between 0 to N-1 with probabilities given by the weights\n",
    "    samp_inds = np.random.choice(rr, Nsamples, p=weights)\n",
    "\n",
    "    # subselect the samples chosen\n",
    "    samples_out = samples[samp_inds, :]\n",
    "\n",
    "    # return uniform weights\n",
    "    weights_out = np.ones((Nsamples))/Nsamples\n",
    "    return samples_out, weights_out\n",
    "\n",
    "\n",
    "def step(prop, proppdf, current_samples, current_weights, likelihood, data, propagator):\n",
    "    \"\"\"\n",
    "    Propagate a particle filter\n",
    "\n",
    "    Inputs\n",
    "    --------\n",
    "    prop            - proposal function (current_state, data)\n",
    "    proppdf         - proposal function logpdf\n",
    "    current_samples - ensemble of samples\n",
    "    current_weights - ensemble of weights\n",
    "    likelihood      - function to evaluate the log likelihood (samples, data)\n",
    "    data            - Observation\n",
    "    propagator      - dynamics logpdf\n",
    "\n",
    "    @returns samples and weights after assimilating the data\n",
    "    \"\"\"\n",
    "\n",
    "    new_samples = prop(current_samples, data)\n",
    "    new_weights = likelihood(new_samples, data) + propagator(new_samples, current_samples) - \\\n",
    "        proppdf(new_samples, current_samples, data)\n",
    "    new_weights = np.exp(new_weights) * current_weights\n",
    "\n",
    "    # normalize weights\n",
    "    new_weights = new_weights / np.sum(new_weights)\n",
    "    return new_samples, new_weights\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PFResults:\n",
    "    delta: float\n",
    "    R: float\n",
    "    samples: np.ndarray\n",
    "    weights: np.ndarray\n",
    "    eff: np.ndarray\n",
    "\n",
    "\n",
    "def particle_filter(data, prior, prop, proppdf, likelihood, propagator,\n",
    "                    nsamples=1000, resampling_threshold_frac=0.1):\n",
    "    \"\"\"Particle Filter\n",
    "\n",
    "    Inputs\n",
    "    -------\n",
    "    data: (nsteps, m) array of data points, N is the time index, m is the dimensionality of the data\n",
    "    prior_mean: (d), prior mean\n",
    "    prior_cov: (d, d), prior mean\n",
    "    Nsamples: integer, number of samples in the empirical distribution\n",
    "    resampling_threshold_frac: float between 0 and 1 indicating to resample when effective sample size below frac of nsamples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    samples: (nsamples, d, nsteps)\n",
    "    weights: (nsamples, nsteps)\n",
    "    eff: (nsamples), effective sample size\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For documentation of prop, proppdf, likelihood, and propagator -- see the step function\n",
    "    \"\"\"\n",
    "\n",
    "    d = prior.mean.shape[0]\n",
    "    nsteps = data.times.shape[0]\n",
    "\n",
    "    # Allocate memory\n",
    "    samples = np.zeros((nsamples, d, nsteps))\n",
    "    weights = np.zeros((nsamples, nsteps))\n",
    "    eff = np.zeros((nsteps))  # keep track of effective sample size at each step\n",
    "\n",
    "    # Generate initial samples from the prior\n",
    "    L = np.linalg.cholesky(prior.cov)\n",
    "    samples[:, :, 0] = np.tile(prior.mean, (nsamples, 1)) + \\\n",
    "        np.dot(L, np.random.randn(d, nsamples)).T\n",
    "    # all weights are equal because of independent sampling from prior\n",
    "    weights[:, 0] = 1.0 / nsamples\n",
    "    eff[0] = nsamples\n",
    "\n",
    "    resamp_threshold = int(nsamples * resampling_threshold_frac)\n",
    "\n",
    "    on_obs = 0\n",
    "\n",
    "    for ii in range(1, nsteps):\n",
    "        if on_obs < data.obs_ind.shape[0] and ii == data.obs_ind[on_obs]:\n",
    "            samples[:, :, ii], weights[:, ii] = step(prop,  proppdf, samples[:, :, ii-1], weights[:, ii-1],\n",
    "                                                     likelihood, data.obs[on_obs, :], propagator)\n",
    "            on_obs += 1\n",
    "        else:\n",
    "            samples[:, :, ii] = dyn_prop(samples[:, :, ii-1])\n",
    "            weights[:, ii] = weights[:, ii-1]\n",
    "\n",
    "        # compute the effective sample size\n",
    "        eff[ii] = 1.0 / np.sum(weights[:, ii]**2)\n",
    "\n",
    "        if ii % 50 == 0:\n",
    "            print(\"eff = \", ii, eff[ii])\n",
    "        # print(\"eff/emean = \", eff[ii], means[ii, :])\n",
    "\n",
    "        # resample if effective sample size is below threshold\n",
    "        if eff[ii] < resamp_threshold:\n",
    "            samples[:, :, ii], weights[:, ii] = resample(\n",
    "                nsamples, samples[:, :, ii], weights[:, ii])\n",
    "\n",
    "    return samples, weights, eff\n",
    "\n",
    "\n",
    "def dyn_prop(current_state, data=None, dt=DT):\n",
    "    \"\"\" Bootstrap Particle Filter the proposal is the dynamics!\"\"\"\n",
    "\n",
    "    if current_state.ndim == 1:\n",
    "        return xkp1_vect(current_state, dt=dt) + np.dot(proc_cov_sqrtL, np.random.randn(2))\n",
    "    else:\n",
    "        nsamples = current_state.shape[0]\n",
    "        return xkp1_vect(current_state, dt=dt) + np.dot(proc_cov_sqrtL, np.random.randn(2, nsamples)).T\n",
    "\n",
    "\n",
    "def dyn_prop_logpdf(current, previous, data=None, proc_var=0.1):\n",
    "    \"\"\" Bootstrap Particle Filter: the proposal is the dynamics\"\"\"\n",
    "    nexts = xkp1_vect(previous, dt=DT)\n",
    "    delta = nexts - current\n",
    "    if current.ndim == 1:\n",
    "        return -0.5 * np.dot(delta, np.dot(proc_cov_inv, delta))\n",
    "    else:\n",
    "        return -0.5 * np.sum(delta * np.dot(delta, proc_cov_inv.T), axis=1)\n",
    "\n",
    "\n",
    "def likelihood(state, data, noise_var):\n",
    "    \"\"\"Gaussian Likelihood through nonlinear model\"\"\"\n",
    "    dpropose = yk_vect(state)\n",
    "    delta = dpropose - data\n",
    "    if state.ndim == 1:\n",
    "        return -0.5 * np.dot(delta, delta) / noise_var\n",
    "    else:\n",
    "        return -0.5 * np.sum(delta * delta, axis=1) / noise_var\n",
    "\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "pf_results = []\n",
    "pf_start = time.time()\n",
    "data_sel = [data[0], data[10], data[15]]\n",
    "for obs in data_sel:\n",
    "    print(f'PF delta={obs.delta} R={obs.R}')\n",
    "    def lk(state, data): return likelihood(state, data, noise_var=obs.R)\n",
    "    samples, weights, eff = particle_filter(\n",
    "        obs.obs, prior, dyn_prop, dyn_prop_logpdf, lk, dyn_prop_logpdf, nsamples=1000, resampling_threshold_frac=0.1)\n",
    "    pf_results.append(PFResults(obs.delta, obs.R, samples, weights, eff))\n",
    "print(f'PF took {time.time() - pf_start:.3f} seconds')\n",
    "\n",
    "# for result, obs in zip(pf_results, data):\n",
    "#     mse = mean_sq_error(truth, result)\n",
    "#     print(f'delta={obs.delta} \\tR={obs.R}\\tMSE={mse:.1e}')\n",
    "\n",
    "# fignum = plot_data_and_truth(fignum, 'Pendulum trajectories with PF', truth, data, pf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(samples, weights):\n",
    "    \"\"\"Compute the mean and standard deviation of multiple empirical distirbution.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    samples: (N, d, m) array of samples defining the empirical distribution\n",
    "    weights: (N, m) array of weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    means: (m, d) array of means\n",
    "    stds: (m, d) array of standard deviations\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    m is the number of empirical distributions\n",
    "    \"\"\"\n",
    "\n",
    "    N, d, m = samples.shape\n",
    "    means = np.zeros((m, d))\n",
    "    stds = np.zeros((m, d))\n",
    "    for ii in range(m):\n",
    "        means[ii, :] = np.dot(weights[:, ii], samples[:, :, ii])\n",
    "        stds[ii, :] = np.sqrt(\n",
    "            np.dot(weights[:, ii], (samples[:, :, ii] - np.tile(means[ii, :], (N, 1)))**2))\n",
    "    return means, stds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "567repo-ITb6N_oG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
